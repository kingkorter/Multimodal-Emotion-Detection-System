{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T11:21:29.182881Z","iopub.status.busy":"2025-04-29T11:21:29.182581Z","iopub.status.idle":"2025-04-29T11:22:04.298473Z","shell.execute_reply":"2025-04-29T11:22:04.297221Z","shell.execute_reply.started":"2025-04-29T11:21:29.182854Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-04-29 11:21:47.709639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1745925707.986727      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1745925708.071773      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["Is CUDA available? False\n"]},{"ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1095213405.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Is CUDA available?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Device name:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of GPUs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \"\"\"\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}],"source":["import torch \n","import torch.nn as nn \n","import torchvision.models as models\n","from transformers import Wav2Vec2Model\n","import torchaudio\n","import cv2\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","import os\n","import torch.optim as optim\n","from tqdm import tqdm\n","from transformers import Wav2Vec2Processor\n","import os\n","from pathlib import Path\n","from sklearn.model_selection import train_test_split\n","import cv2\n","import glob\n","import random\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Is CUDA available?\", torch.cuda.is_available())\n","print(\"Device name:\", torch.cuda.get_device_name(0))\n","print(\"Number of GPUs:\", torch.cuda.device_count())"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T11:25:44.499573Z","iopub.status.busy":"2025-04-29T11:25:44.499232Z","iopub.status.idle":"2025-04-29T11:25:44.505422Z","shell.execute_reply":"2025-04-29T11:25:44.504584Z","shell.execute_reply.started":"2025-04-29T11:25:44.499548Z"},"trusted":true},"outputs":[],"source":["from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader, random_split, Dataset\n","from torchvision.models import resnet50, ResNet50_Weights\n","\n","\n","transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T11:25:49.629439Z","iopub.status.busy":"2025-04-29T11:25:49.629138Z","iopub.status.idle":"2025-04-29T11:26:09.176318Z","shell.execute_reply":"2025-04-29T11:26:09.175440Z","shell.execute_reply.started":"2025-04-29T11:25:49.629398Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:61: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9b765552c314e7b89a22133a36cbe7d","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b48621ba9cac4a2ea90cede5651405da","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31516b6b5345482783e104764b68bedd","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01929d711440462eb293876a3d93dcb1","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"427963a6910549d0a0df5ff2a19b759d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n","  warnings.warn(\n"]}],"source":["processor= Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T11:26:24.745770Z","iopub.status.busy":"2025-04-29T11:26:24.745453Z","iopub.status.idle":"2025-04-29T11:26:24.750391Z","shell.execute_reply":"2025-04-29T11:26:24.749313Z","shell.execute_reply.started":"2025-04-29T11:26:24.745746Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader, random_split, Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-04-28T19:22:28.395216Z","iopub.status.busy":"2025-04-28T19:22:28.394702Z","iopub.status.idle":"2025-04-28T19:22:28.402132Z","shell.execute_reply":"2025-04-28T19:22:28.401446Z","shell.execute_reply.started":"2025-04-28T19:22:28.395191Z"},"trusted":true},"outputs":[],"source":["\n","def extract_frame(video_folder, output_folder):\n","    os.makedirs(output_folder, exist_ok =True)\n","    video_files=glob.glob((os.path.join(video_folder, \"*.flv\")))\n","    for video_path in video_files:\n","        video_name = os.path.basename(video_path).replace(\".flv\", \"\")\n","        output_path= os.path.join(output_folder, video_name)\n","        os.makedirs(output_path, exist_ok= True)\n","        cap =cv2.VideoCapture(video_path)\n","        frame_count=0\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            if frame_count % 10 == 0:\n","               frame_path = os.path.join(output_path, f\"frame_{frame_count:04d}.jpg\")\n","               cv2.imwrite(frame_path, frame)\n","            frame_count +=1\n","        cap.release()\n","    print(f\"Extracted frames to {output_folder}\")\n","    !zip -r faces.zip /kaggle/working/faces\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"execution_failed":"2025-04-28T23:37:17.094Z"},"trusted":true},"outputs":[],"source":["extract_frame(\"/kaggle/input/crema-d-video\", \"/kaggle/working/faces\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T04:57:48.271746Z","iopub.status.busy":"2025-04-29T04:57:48.270972Z","iopub.status.idle":"2025-04-29T04:57:48.279088Z","shell.execute_reply":"2025-04-29T04:57:48.278188Z","shell.execute_reply.started":"2025-04-29T04:57:48.271715Z"},"trusted":true},"outputs":[],"source":["face_root_folder= \"/kaggle/input/faces-dataset/kaggle/working/faces\"\n","audio_root_folder= \"/kaggle/input/cremad/AudioWAV\"\n","def find_audio_files_and_labels(folder_path):\n","    audio_files = []\n","    labels = []\n","    audio_extensions = (\".wav\", \".mp3\", \".flac\")\n","    emotion_map = {\n","        \"HAP\": 0,  # Happy\n","        \"SAD\": 1,  # Sad\n","        \"ANG\": 2,  # Angry\n","        \"FEA\": 3,  # Fear\n","        \"DIS\": 4,  # Disgust\n","        \"NEU\": 5   # Neutral\n","    }\n","    for root, dirs, files in os.walk(folder_path):\n","        for file in files:\n","            if file.lower().endswith(audio_extensions):\n","                audio_files.append(os.path.join(root, file))\n","                # Extract emotion from file name (e.g., 1001_DFA_ANG_XX.wav)\n","                parts = file.split(\"_\")\n","                if len(parts) >= 3:\n","                    emotion_code = parts[2]\n","                    label = emotion_map.get(emotion_code, 5)  # Default to Neutral\n","                else:\n","                    label = 5\n","                labels.append(label)\n","    return audio_files, labels"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T04:58:19.376210Z","iopub.status.busy":"2025-04-29T04:58:19.375833Z","iopub.status.idle":"2025-04-29T04:58:19.382648Z","shell.execute_reply":"2025-04-29T04:58:19.381849Z","shell.execute_reply.started":"2025-04-29T04:58:19.376181Z"},"trusted":true},"outputs":[],"source":["# Find face image files and labels\n","def find_face_files_and_labels(folder_path):\n","    face_files = []\n","    labels = []\n","    image_extensions = (\".jpg\", \".jpeg\", \".png\")\n","    emotion_map = {\n","        \"HAP\": 0,  # Happy\n","        \"SAD\": 1,  # Sad\n","        \"ANG\": 2,  # Angry\n","        \"FEA\": 3,  # Fear\n","        \"DIS\": 4,  # Disgust\n","        \"NEU\": 5   # Neutral\n","    }\n","    for root, dirs, files in os.walk(folder_path):\n","        for file in files:\n","            if file.lower().endswith(image_extensions):\n","                face_files.append(os.path.join(root, file))\n","                # Extract emotion from parent folder (e.g., 1001_DFA_ANG_XX)\n","                folder_name = os.path.basename(root)\n","                parts = folder_name.split(\"_\")\n","                if len(parts) >= 3:\n","                    emotion_code = parts[2]\n","                    label = emotion_map.get(emotion_code, 5)\n","                else:\n","                    label = 5\n","                labels.append(label)\n","    return face_files, labels"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T05:00:22.748327Z","iopub.status.busy":"2025-04-29T05:00:22.747740Z","iopub.status.idle":"2025-04-29T05:00:22.753872Z","shell.execute_reply":"2025-04-29T05:00:22.753046Z","shell.execute_reply.started":"2025-04-29T05:00:22.748307Z"},"trusted":true},"outputs":[],"source":["# Pair audio and face files\n","def pair_audio_face_files(audio_files, face_root_folder):\n","    paired_audio = []\n","    paired_face = []\n","    labels = []\n","    emotion_map = {\n","        \"HAP\": 0,  # Happy\n","        \"SAD\": 1,  # Sad\n","        \"ANG\": 2,  # Angry\n","        \"FEA\": 3,  # Fear\n","        \"DIS\": 4,  # Disgust\n","        \"NEU\": 5   # Neutral\n","    }\n","    for audio_file in audio_files:\n","        audio_name = os.path.basename(audio_file).replace(\".wav\", \"\")\n","        face_folder = os.path.join(face_root_folder, audio_name)\n","        face_files = glob.glob(os.path.join(face_folder, \"*.jpg\"))\n","        if face_files:  # If matching face frames exist\n","            # Use a random frame for variety\n","            paired_audio.append(audio_file)\n","            paired_face.append(random.choice(face_files))  # Random frame\n","            parts = audio_name.split(\"_\")\n","            if len(parts) >= 3:\n","                emotion_code = parts[2]\n","                label = emotion_map.get(emotion_code, 5)\n","            else:\n","                label = 5\n","            labels.append(label)\n","    return paired_audio, paired_face, labels"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T05:00:27.481766Z","iopub.status.busy":"2025-04-29T05:00:27.481076Z","iopub.status.idle":"2025-04-29T05:01:14.762832Z","shell.execute_reply":"2025-04-29T05:01:14.762234Z","shell.execute_reply.started":"2025-04-29T05:00:27.481741Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 7442 audio files\n","Found 60367 face images\n","Found 7441 paired audio-face samples\n"]}],"source":["audio_files, audio_labels = find_audio_files_and_labels(audio_root_folder)\n","print(f\"Found {len(audio_files)} audio files\")\n","face_files, face_labels = find_face_files_and_labels(face_root_folder)\n","print(f\"Found {len(face_files)} face images\")\n","paired_audio_files, paired_face_files, paired_labels =pair_audio_face_files(audio_files, face_root_folder)\n","print(f\"Found {len(paired_audio_files)} paired audio-face samples\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T05:01:22.235018Z","iopub.status.busy":"2025-04-29T05:01:22.234423Z","iopub.status.idle":"2025-04-29T05:01:22.271091Z","shell.execute_reply":"2025-04-29T05:01:22.270387Z","shell.execute_reply.started":"2025-04-29T05:01:22.234996Z"},"trusted":true},"outputs":[],"source":["# Train-test split\n","voice_train_files, voice_test_files, voice_train_labels, voice_test_labels = train_test_split(\n","    audio_files, audio_labels, test_size=0.2, random_state=42\n",")\n","face_train_files, face_test_files, face_train_labels, face_test_labels = train_test_split(\n","    face_files, face_labels, test_size=0.2, random_state=42\n",")\n","paired_train_audio, paired_test_audio, paired_train_face, paired_test_face, paired_train_labels, paired_test_labels = train_test_split(\n","    paired_audio_files, paired_face_files, paired_labels, test_size=0.2, random_state=42\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T05:01:34.482718Z","iopub.status.busy":"2025-04-29T05:01:34.482383Z","iopub.status.idle":"2025-04-29T05:01:34.489573Z","shell.execute_reply":"2025-04-29T05:01:34.488935Z","shell.execute_reply.started":"2025-04-29T05:01:34.482690Z"},"trusted":true},"outputs":[],"source":["class VoiceDataset(Dataset):\n","    def __init__(self, audio_files,labels,processor):\n","        self.audio_files = audio_files\n","        self.labels = labels\n","        self.processor = processor\n","    def __len__(self):\n","        return len(self.audio_files)\n","    def __getitem__(self, index):\n","      try: \n","        waveform, sample_rate=  torchaudio.load(self.audio_files[index])\n","        if waveform.shape[0] > 1:\n","            waveform = waveform.mean(dim=0, keepdim=True)  # Average channels\n","        if sample_rate != 16000:\n","            resampler = torchaudio.transforms.Resample(orig_freq= sample_rate, new_freq= 16000)\n","            waveform= resampler(waveform)\n","        desired_length = 16000 * 4  \n","\n","        if waveform.shape[1] < desired_length:\n","              pad_size = desired_length - waveform.shape[1]\n","              waveform = torch.nn.functional.pad(waveform, (0, pad_size))\n","        else:\n","               waveform = waveform[:, :desired_length]\n","        # Handle multi-channel audio\n","       \n","        \n","        inputs = self.processor(\n","                waveform.squeeze(0), sampling_rate=16000, return_tensors=\"pt\", padding=True\n","            )\n","        \n","        out= {\n","                \"input_values\": inputs.input_values.squeeze(0),\n","                \"label\": torch.tensor(self.labels[index])\n","            }\n","        if hasattr(inputs, \"attention_mask\") and inputs.attention_mask is not None:\n","              out[\"attention_mask\"] = inputs.attention_mask.squeeze(0)\n","        return out\n","            \n","\n","           \n","        \n","      except Exception as e:\n","             print(f\"Error processing {self.audio_files[index]}: {e}\")\n","             raise e"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T05:01:38.574392Z","iopub.status.busy":"2025-04-29T05:01:38.574119Z","iopub.status.idle":"2025-04-29T05:01:38.579538Z","shell.execute_reply":"2025-04-29T05:01:38.578890Z","shell.execute_reply.started":"2025-04-29T05:01:38.574374Z"},"trusted":true},"outputs":[],"source":["class FaceDataset(Dataset):\n","    def __init__(self, face_files, labels, transform):\n","        self.face_files = face_files\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.face_files)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            image = Image.open(self.face_files[idx]).convert(\"RGB\")\n","            image = self.transform(image)\n","            return {\n","                \"image\": image,\n","                \"label\": torch.tensor(self.labels[idx])\n","            }\n","        except Exception as e:\n","            print(f\"Error processing {self.face_files[idx]}: {e}\")\n","            return None"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T05:01:41.874565Z","iopub.status.busy":"2025-04-29T05:01:41.874269Z","iopub.status.idle":"2025-04-29T05:01:41.881940Z","shell.execute_reply":"2025-04-29T05:01:41.881251Z","shell.execute_reply.started":"2025-04-29T05:01:41.874545Z"},"trusted":true},"outputs":[],"source":["class PairedDataset(Dataset):\n","    def __init__(self, audio_files, face_files, labels, processor, transform):\n","        self.audio_files = audio_files\n","        self.face_files = face_files\n","        self.labels = labels\n","        self.processor = processor\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.audio_files)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            # Audio\n","            waveform, sample_rate = torchaudio.load(self.audio_files[idx])\n","            if sample_rate != 16000:\n","                waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n","            if waveform.shape[0] > 1:\n","                waveform = waveform.mean(dim=0, keepdim=True)\n","            desired_length = 16000 * 4  \n","\n","            if waveform.shape[1] < desired_length:\n","              pad_size = desired_length - waveform.shape[1]\n","              waveform = torch.nn.functional.pad(waveform, (0, pad_size))\n","            else:\n","               waveform = waveform[:, :desired_length]\n","            inputs = self.processor(\n","                waveform.squeeze(0), sampling_rate=16000, return_tensors=\"pt\", padding=True\n","            )\n","            # Image\n","            image = Image.open(self.face_files[idx]).convert(\"RGB\")\n","            image = self.transform(image)\n","            output= {\n","                \"input_values\": inputs.input_values.squeeze(0),\n","                # \"attention_mask\": inputs.attention_mask.squeeze(0) if inputs.attention_mask is not None else None,\n","                \"image\": image,\n","                \"label\": torch.tensor(self.labels[idx])\n","            }\n","            if hasattr(inputs, \"attention_mask\") and inputs.attention_mask is not None:\n","              output[\"attention_mask\"] = inputs.attention_mask.squeeze(0)\n","            return output\n","        except Exception as e:\n","            print(f\"Error processing {self.audio_files[idx]} or {self.face_files[idx]}: {e}\")\n","            return None"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T05:01:46.448922Z","iopub.status.busy":"2025-04-29T05:01:46.448244Z","iopub.status.idle":"2025-04-29T05:01:46.454425Z","shell.execute_reply":"2025-04-29T05:01:46.453727Z","shell.execute_reply.started":"2025-04-29T05:01:46.448901Z"},"trusted":true},"outputs":[],"source":["\n","# Create dataloaders\n","voice_train_dataset = VoiceDataset(voice_train_files, voice_train_labels, processor)\n","voice_test_dataset = VoiceDataset(voice_test_files, voice_test_labels, processor)\n","face_train_dataset = FaceDataset(face_train_files, face_train_labels, transform)\n","face_test_dataset = FaceDataset(face_test_files, face_test_labels, transform)\n","paired_train_dataset = PairedDataset(paired_train_audio, paired_train_face, paired_train_labels, processor, transform)\n","paired_test_dataset = PairedDataset(paired_test_audio, paired_test_face, paired_test_labels, processor, transform)\n","\n","voice_train_dataloader = DataLoader(voice_train_dataset, batch_size=2, shuffle=True)\n","voice_test_dataloader = DataLoader(voice_test_dataset, batch_size=2, shuffle=False)\n","face_train_dataloader = DataLoader(face_train_dataset, batch_size=32, shuffle=True)\n","face_test_dataloader = DataLoader(face_test_dataset, batch_size=32, shuffle=False)\n","paired_train_dataloader = DataLoader(paired_train_dataset, batch_size=32, shuffle=True)\n","paired_test_dataloader = DataLoader(paired_test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T11:26:44.918155Z","iopub.status.busy":"2025-04-29T11:26:44.917781Z","iopub.status.idle":"2025-04-29T11:26:44.927060Z","shell.execute_reply":"2025-04-29T11:26:44.925671Z","shell.execute_reply.started":"2025-04-29T11:26:44.918129Z"},"trusted":true},"outputs":[],"source":["class FaceEmotionModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.resnet= models.resnet50(weights=ResNet50_Weights.DEFAULT)\n","        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 6)\n","        for name, param in self.resnet.named_parameters():\n","            if \"layer4\" in name or \"fc\" in name:\n","                param.requires_grad = True\n","            else:\n","                param.requires_grad = False\n","        \n","    def forward(self, images):\n","        return self.resnet(images)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["help(nn.Module)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T11:26:49.114304Z","iopub.status.busy":"2025-04-29T11:26:49.113986Z","iopub.status.idle":"2025-04-29T11:26:49.121017Z","shell.execute_reply":"2025-04-29T11:26:49.120049Z","shell.execute_reply.started":"2025-04-29T11:26:49.114279Z"},"trusted":true},"outputs":[],"source":["class VoiceEmotionModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n","        self.classifier= nn.Linear(self.wav2vec.config.hidden_size, 6)\n","        for name, param in self.wav2vec.named_parameters():\n","            if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n","                param.requires_grad = True  # UNFREEZE\n","            else:\n","                param.requires_grad = False  # KEEP FROZEN\n","\n","    def forward(self, input_values, attention_mask=None):\n","        outputs = self.wav2vec(input_values = input_values, attention_mask=attention_mask)\n","        hidden_states= outputs.last_hidden_state\n","        pooled = hidden_states.mean(dim=1)\n","        return self.classifier(pooled)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T11:26:52.862798Z","iopub.status.busy":"2025-04-29T11:26:52.861971Z","iopub.status.idle":"2025-04-29T11:26:52.868873Z","shell.execute_reply":"2025-04-29T11:26:52.867846Z","shell.execute_reply.started":"2025-04-29T11:26:52.862767Z"},"trusted":true},"outputs":[],"source":["#Combined Model Class\n","class CombinedEmotionModel(nn.Module):\n","    def __init__(self,voice_model,face_model):\n","        super().__init__()\n","        self.face_model= face_model\n","        self.voice_model= voice_model\n","        self.fusion= nn.Linear(12,6)\n","    def forward(self,*, input_values=None, attention_mask=None, images=None):\n","        \n","        voice_output = self.voice_model.forward(input_values=input_values, attention_mask=attention_mask)\n","        face_output = self.face_model(images)\n","        \n","\n","        combined =torch.cat((voice_output, face_output), dim =1)\n","        return self.fusion(combined)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T05:04:48.128464Z","iopub.status.busy":"2025-04-29T05:04:48.127771Z","iopub.status.idle":"2025-04-29T05:04:51.907702Z","shell.execute_reply":"2025-04-29T05:04:51.906779Z","shell.execute_reply.started":"2025-04-29T05:04:48.128423Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e41ffaa93d1e4a65a9889400e70ec0ac","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5da9d3bcc02b4184b94b15890942d8a3","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","\n","  0%|          | 0.00/97.8M [00:00<?, ?B/s]\u001b[A\n"," 12%|█▏        | 11.9M/97.8M [00:00<00:00, 124MB/s]\u001b[A\n"," 30%|██▉       | 28.9M/97.8M [00:00<00:00, 156MB/s]\u001b[A\n"," 47%|████▋     | 46.1M/97.8M [00:00<00:00, 167MB/s]\u001b[A\n"," 64%|██████▎   | 62.1M/97.8M [00:00<00:00, 165MB/s]\u001b[A\n"," 80%|███████▉  | 78.0M/97.8M [00:00<00:00, 163MB/s]\u001b[A\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 162MB/s]\u001b[A\n"]}],"source":["voice_model = VoiceEmotionModel().to(device)\n","face_model = FaceEmotionModel().to(device)\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T05:34:44.941215Z","iopub.status.busy":"2025-04-29T05:34:44.940936Z","iopub.status.idle":"2025-04-29T05:34:44.947220Z","shell.execute_reply":"2025-04-29T05:34:44.946579Z","shell.execute_reply.started":"2025-04-29T05:34:44.941195Z"},"trusted":true},"outputs":[],"source":["# Define loss and optimizers\n","criterion = nn.CrossEntropyLoss()\n","\n","face_optimizer = optim.Adam(face_model.parameters(), lr=1e-3)\n","combined_optimizer = optim.Adam(combined_model.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T07:08:02.847347Z","iopub.status.busy":"2025-04-29T07:08:02.846587Z","iopub.status.idle":"2025-04-29T07:08:02.852034Z","shell.execute_reply":"2025-04-29T07:08:02.851182Z","shell.execute_reply.started":"2025-04-29T07:08:02.847322Z"},"trusted":true},"outputs":[],"source":["voice_optimizer = optim.Adam(voice_model.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T07:08:11.290279Z","iopub.status.busy":"2025-04-29T07:08:11.289988Z","iopub.status.idle":"2025-04-29T08:15:23.190649Z","shell.execute_reply":"2025-04-29T08:15:23.189912Z","shell.execute_reply.started":"2025-04-29T07:08:11.290260Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Voice Epoch 1, Loss: 1.3702092512836384\n","Voice Epoch 2, Loss: 1.278234402956\n","Voice Epoch 3, Loss: 1.2240777473232272\n","Voice Epoch 4, Loss: 1.195454100706872\n","Voice Epoch 5, Loss: 1.176544658556896\n","Voice Epoch 6, Loss: 1.1626032928494767\n","Voice Epoch 7, Loss: 1.1467813611841458\n","Voice Epoch 8, Loss: 1.1393870631502525\n","Voice Epoch 9, Loss: 1.125921913915366\n","Voice Epoch 10, Loss: 1.1089906319137444\n"]}],"source":["# Train Voice Model (pre-training for better fusion)\n","\n","voice_model.train()\n","for epoch in range(10):\n","    total_loss=0\n","    for batch in voice_train_dataloader:\n","        input_values = batch[\"input_values\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device) if \"attention_mask\" in batch else None\n","        labels = batch[\"label\"].to(device)\n","        outputs = voice_model(input_values, attention_mask)\n","        loss = criterion(outputs, labels)\n","\n","        voice_optimizer.zero_grad()\n","        loss.backward()\n","        voice_optimizer.step()\n","        total_loss += loss.item()\n","    avg_loss = total_loss / len(voice_train_dataloader)\n","    print(f\"Voice Epoch {epoch+1}, Loss: {avg_loss}\")"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T08:15:57.258803Z","iopub.status.busy":"2025-04-29T08:15:57.258463Z","iopub.status.idle":"2025-04-29T08:16:30.034087Z","shell.execute_reply":"2025-04-29T08:16:30.033261Z","shell.execute_reply.started":"2025-04-29T08:15:57.258773Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Voice Model Test Accuracy: 0.4533\n"]}],"source":["# Test Voice Model \n","voice_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for batch in voice_test_dataloader:\n","        input_values = batch[\"input_values\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device) if \"attention_mask\" in batch else None\n","        labels = batch[\"label\"].to(device)\n","\n","        outputs = voice_model(input_values, attention_mask)\n","        preds = torch.argmax(outputs, dim=1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","voice_accuracy = correct / total\n","print(f\"Voice Model Test Accuracy: {voice_accuracy:.4f}\")\n","torch.save(voice_model.state_dict(), \"voice_emotion_model.pth\")"]},{"cell_type":"code","execution_count":31,"metadata":{"_kg_hide-output":false,"execution":{"iopub.execute_input":"2025-04-29T06:39:18.759687Z","iopub.status.busy":"2025-04-29T06:39:18.759426Z","iopub.status.idle":"2025-04-29T07:02:09.542007Z","shell.execute_reply":"2025-04-29T07:02:09.541350Z","shell.execute_reply.started":"2025-04-29T06:39:18.759665Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Face Epoch 1, Loss: 1695.790233194828\n","Face Epoch 2, Loss: 1101.8616681396961\n","Face Epoch 3, Loss: 742.527104511857\n"]}],"source":["# Train Face Model \n","\n","face_model.train()\n","for epoch in range(3):\n","    total_loss = 0\n","    for batch in face_train_dataloader:\n","        images = batch[\"image\"].to(device)\n","        labels = batch[\"label\"].to(device)\n","\n","        outputs = face_model(images)\n","        loss = criterion(outputs, labels)\n","\n","        face_optimizer.zero_grad()\n","        loss.backward()\n","        face_optimizer.step() \n","        total_loss += loss.item()\n","\n","    print(f\"Face Epoch {epoch+1}, Loss: {total_loss}\")"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T07:03:27.399104Z","iopub.status.busy":"2025-04-29T07:03:27.398833Z","iopub.status.idle":"2025-04-29T07:05:21.504887Z","shell.execute_reply":"2025-04-29T07:05:21.504098Z","shell.execute_reply.started":"2025-04-29T07:03:27.399086Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Face Model Test Accuracy: 0.7666\n"]}],"source":["# Test Face Model \n","face_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for batch in face_test_dataloader:\n","        images = batch[\"image\"].to(device)\n","        labels = batch[\"label\"].to(device)\n","\n","        outputs = face_model(images)\n","        preds = torch.argmax(outputs, dim=1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","face_accuracy = correct / total\n","print(f\"Face Model Test Accuracy: {face_accuracy:.4f}\")\n","torch.save(face_model.state_dict(), \"face_emotion_model.pth\")"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T08:39:30.331891Z","iopub.status.busy":"2025-04-29T08:39:30.331604Z","iopub.status.idle":"2025-04-29T08:39:30.340797Z","shell.execute_reply":"2025-04-29T08:39:30.340085Z","shell.execute_reply.started":"2025-04-29T08:39:30.331871Z"},"trusted":true},"outputs":[],"source":["combined_model = CombinedEmotionModel(voice_model, face_model).to(device)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T08:44:21.714350Z","iopub.status.busy":"2025-04-29T08:44:21.713570Z","iopub.status.idle":"2025-04-29T08:58:49.449374Z","shell.execute_reply":"2025-04-29T08:58:49.448489Z","shell.execute_reply.started":"2025-04-29T08:44:21.714324Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Combined Epoch 1, Loss: 163.02335065603256\n","Combined Epoch 2, Loss: 73.89563891291618\n","Combined Epoch 3, Loss: 40.71809824183583\n"]}],"source":["# Train Combined Model \n","\n","combined_model.train()\n","for epoch in range(3):\n","    loss_total = 0\n","    for batch in paired_train_dataloader:\n","        input_values = batch[\"input_values\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device) if \"attention_mask\" in batch else None\n","        images = batch[\"image\"].to(device)\n","        labels = batch[\"label\"].to(device)\n","\n","        outputs = combined_model(input_values=input_values, attention_mask=attention_mask, images=images)\n","        loss = criterion(outputs, labels)\n","\n","        combined_optimizer.zero_grad()\n","        loss.backward()\n","        combined_optimizer.step()\n","        loss_total += loss.item()\n","        \n","        \n","\n","    print(f\"Combined Epoch {epoch+1}, Loss: {loss_total}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["help(torch.Size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T08:59:49.308164Z","iopub.status.busy":"2025-04-29T08:59:49.307813Z","iopub.status.idle":"2025-04-29T09:00:33.507061Z","shell.execute_reply":"2025-04-29T09:00:33.506397Z","shell.execute_reply.started":"2025-04-29T08:59:49.308141Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Combined Model Test Accuracy: 0.7965\n"]}],"source":["# Test Combined Model\n","combined_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for batch in paired_test_dataloader:\n","        input_values = batch[\"input_values\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device) if \"attention_mask\" in batch else None\n","        images = batch[\"image\"].to(device)\n","        labels = batch[\"label\"].to(device)\n","\n","        outputs = combined_model(input_values=input_values, attention_mask=attention_mask, images=images)\n","        preds = torch.argmax(outputs, dim=1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","combined_accuracy = correct / total\n","print(f\"Combined Model Test Accuracy: {combined_accuracy:.4f}\")\n","torch.save(combined_model.state_dict(), \"combined_emotion_model.pth\")\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":325566,"sourceId":653195,"sourceType":"datasetVersion"},{"datasetId":3474786,"sourceId":6070990,"sourceType":"datasetVersion"},{"datasetId":7280688,"sourceId":11607651,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":322647,"modelInstanceId":302149,"sourceId":364125,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":31011,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"vscode":{"interpreter":{"hash":"40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"}}},"nbformat":4,"nbformat_minor":4}
